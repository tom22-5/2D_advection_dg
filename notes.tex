\documentclass[12pt]{report}
%%% Packages
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subcaption}
%%%

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

%%%
\setlength{\parindent}{0pt}


\begin{document}

\title{Tensor-Product Preconditioner For Very High Order Discontinuous Galerkin Discretizations}
\author{Tom Feldhausen}
\date{\today}

\maketitle

\begin{abstract}
Solving a multi-dimensional PDE by means of a Discontinuous Galerkin method in a higher order solution space requires the solution of large linear systems during time integration. When storing the semi-discrete system, communication dominates the computational costs such that matrix-free implementations~\cite{kronbichler2019fast} often turn out to be more efficient on modern architectures. In order to efficiently integrate implicit time integration into this framework, we need matrix-free preconditioners. This project aims to compare the tensor-product preconditioners of Diosady~\cite{diosady2017tensor} and Pazner~\cite{pazner2018approximate} for the example of linear advection and provide an parallelized matrix-free C++ deal.II~\cite{arndt2024deal} implemenation. As a first step, we provide a matrix-based DG implementation of two-dimensional linear advection with upwind flux and verify the efficiency of Pazner's preconditioner for implicit time integration.
\end{abstract}

\section*{A Matrix-Based Discontinuous Galerkin Implementation For Two-Dimensional Linear Advection In Python}
\textit{The code for all experiments is provided in \cite{2D_advection_dg}.} \\
As a first step, we provide a matrix-based DG discretization for the two-dimensional linear advection system 
\begin{align*}
&\partial_t u(x, t) + a(x,t) \cdot \nabla u(x,t) = 0 \quad \forall x \in [0,1]^2, \quad 0 \leq t \leq T
\\
&u(x, 0) = u_0(x) \quad \forall x \in [0,1]^2
\\
&u(x, t) = u_{\textnormal{in}}(x) \quad \forall x \in \partial [0, 1]^2
\end{align*}
for a solution function $u$ and a domain $\Omega = [0,1]^2$. Following Kronbichler and Persson~\cite{kronbichler2021efficient}, we arrive at the weak formulation that for all $v \in H^1(\Omega)$ it needs to hold that
\begin{align}
\label{eq:weakform}
\int_\Omega \partial_t u v - \int_\Omega u a \cdot \nabla v + \int_{\partial \Omega} (a \cdot n) u v = 0,
\end{align}
after integrating in space, multiplying by a test function $v$ and integrating by parts.

Now insert tensor product basis functions of order $p$, i.e.
\begin{align*}
u = \sum^{(p+1)^2 - 1}_{i=0} u_i \Phi_i, v = \sum^{(p+1)^2-1}_{j=0} v_j \Phi_j.
\end{align*}
Then condition \eqref{eq:weakform} is equivalent to the element-wise formulation for each basis function, i.e.
\begin{align}
\sum_i \int_{\Omega_e} \partial_t u_i(t) \Phi_i \Phi_j - \int_{\Omega_e} u_i \Phi_i a \cdot \nabla \Phi_j + \int_{\partial \Omega_e} (a \cdot n) u_i \Phi_i \Phi_j = 0
\end{align}
being true for all $0 \leq i,j \leq ^2p$ and $\Omega_e \subset \Omega$.

Analogously we can write this in matrix form, which yields the system
\begin{align}
\label{eq:ode}
\frac{dU(t)}{dt} = M^{-1} [(B + G) U + G_{\textnormal{bound}}].
\end{align}
We consider a tensor-product mesh with $N = N_x \times N_y$ rectangular elements. Then $x: \Omega_{\textnormal{ref}} \rightarrow \Omega_e$ maps from the reference element $\Omega_{\textnormal{ref}} = [-1,1]^2$ to the actual element $\Omega_e = [x_0, x_1] \times [y_0, y_1]$ and can be explicitly written as 
\begin{align*}
\mathbf{x}(\boldsymbol{\zeta})
=
A\,\boldsymbol{\zeta} + \mathbf{b},
\qquad
A = \frac{1}{2}
\begin{pmatrix}
x_1 - x_0 & 0 \\
0 & y_1 - y_0
\end{pmatrix},
\qquad
\mathbf{b} = 
\begin{pmatrix}
\frac{x_1 + x_0}{2} \\
\frac{y_1 + y_0}{2}
\end{pmatrix}.
\end{align*}

With this at hand, we can explicitly write out the mass matrix as
\begin{align*}
M_{ji} = \quad &\int_{\Omega_e} \Phi_j(z) \Phi_i(z) dz
\\ 
= \quad &\int^{x_0}_{x_1} \int^{y_0}_{y_1} \Phi_j(z_0, z_1) \Phi_i(z_0, z_1) dz_0 dz_1 
\\
= \quad &\int^1_{-1} \int^1_{-1} \Phi_j(\zeta_1, \zeta_2)  \Phi_i(\zeta_1, \zeta_2) \|\operatorname{det}(J^{-1}_e)\| d\zeta_1 d\zeta_2.
\end{align*}

Using Gauss-Legendre quadrature with one-dimensional weights $w_q$ and points $x_q$, we get the approximation
\begin{align*}
M_{ji} = \quad &\int_{\Omega_e} \Phi_j(z) \Phi_i(z) dz
\\ 
\approx \quad &\sum_{q_0} \sum_{q_1} w_{q_0} w_{q_1} \phi_{i_1}(x_{q_0}) \phi_{i_2}(x_{q_1}) \phi_{j_1}(x_{q_0}) \phi_{j_2}(x_{q_1}) \frac{1}{4} (x_1 - x_0) (y_1 - y_0)
\end{align*}
when inserting the tensor-product form $\Phi_i = \phi_{i_1} \phi_{i_2}$ and exploiting $\|\operatorname{det}(J^{-1}_e)\| = \frac{1}{4} (x_1 - x_0) (y_1 - y_0)$.

Similarly, we can derive an expression for the volume matrix and calculate
\begin{align*}
B_{ji} = \quad &\int_{\Omega_e} \Phi_i(z) (a \cdot \nabla \Phi_j(z)) dz
\\
= \quad &\int^{x_0}_{x_1} \int^{y_1}_{y_0} \Phi_i(z_0, z_1) (a(z_0, z_1, t) \cdot \nabla \Phi_j(z_0, z_1)) dz_0 dz_1 
\\
= \quad &\int^{-1}_{1} \int^{-1}_{1} \Phi_i(\zeta_0, \zeta_1) (a(z_0, z_1, t) \cdot J_e \nabla \Phi_j(\zeta_0, \zeta_1)) d\zeta_0 d\zeta_1 
\\ 
= \quad &\int^{-1}_{1} \int^{-1}_{1} \Phi_i(\zeta_0, \zeta_1) (2 (x_1 - x_0)^{-1} a_0(z_0, z_1, t) \partial_{\zeta_0} \Phi_j(\zeta_0, \zeta_1) + 2 (y_1 - y_0)^{-1} a_1(z_0, z_1, t) \partial_{\zeta_1} \Phi_i(\zeta_0, \zeta_1)) 
\\ 
\quad &\frac{1}{4} (x_1 - x_0) (y_1 - y_0) d\zeta_0 d\zeta_1
\\
= \quad &\sum_{q_0} \sum_{q_1} w_{q_0} w_{q_1} \phi_{i_1}(x_{q_0}) \phi_{i_2}(x_{q_1}) (a_0(z_{q_0}, z_{q_1}, t) \phi'_{j_1}(x_{q_0}) + a_1(z_{q_0}, z_{q_1}, t) \phi_{j_1}(x_{q_0}) \phi'_{j_2}(x_{q_1}) \phi_{j_2}(x_{q_1}))
\\
\quad &\frac{1}{4} (x_1 - x_0) (y_1 - y_0).
\end{align*}

The face terms are slightly more difficult to derive as in two dimensions we have four faces that are either interior and face other elements or that are part of the domain boundary. This is why for a specific derivation, we need to choose an explicit flux and face. In this example, we choose an upwind flux for information flow between elements, i.e. 
\begin{align*}
\widehat{au}
=
\begin{cases}
(a \cdot n) u^{-}, & a \cdot n > 0, \\[6pt]
(a \cdot n) u^{+}, & a \leq 0
\end{cases}
\end{align*}
and focus on the left face. As this is one-dimensional, we will only need the $x_1(\zeta)$ part of the transformation, that concerns the $x_1$-axis. The corresponding determinant of the inverse Jacobian is simply $(y_1 - y_0)$ for our tensor-product grid. Generally it holds that 
\begin{align*}
&\int_{\partial \Omega_e} \widehat{au}(\Phi_i^R(z), \Phi_i^L(z)) \Phi_j(z) dz
\\
= \quad &\int^1_{-1} (y_1 - y_0) \phi_{j_0}(-1) \phi_{j_1}(\zeta) \widehat{au}(\Phi^R_i(zeta), \Phi^L_j(\zeta)) d\zeta
\\
\approx \quad &\sum_q w_q (y_1 - y_0) \phi_{j_0}(-1) \phi_{j_1}(x_q) \widehat{au}(u^R_i \phi^R_{j_0}(-1) \phi^R_{j_1}(x_1(x_q)), u^L_i \phi^L_{j_0}(-1) \phi^L_{j_1}(x_1(x_q))).
\end{align*}

The value of $\widehat{au}(u^R_i \phi^R_{j_0}(-1) \phi^R_{j_1}(x_1(x_q)), u^L_i \phi^L_{j_0}(-1) \phi^L_{j_1}(x_1(x_q)))$ depends on the following cases. 

\textbf{Case 1A: Inflow Boundary.} \\
\begin{align*}
{G_{\textnormal{bound}}}_j \approx \quad &\sum_q w_q (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) u(x_0(-1), x_1(x_q), t)
\end{align*}

\textbf{Case 1B: Outflow Boundary.} \\
\begin{align*}
G_{ji^R} \approx \quad &\sum_q w_q (y_1 - y_0) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) \phi^R_{i_0}(-1) \phi^R_{j_1}(x_q) u^R_i
\end{align*}

\textbf{Case 2A: Inflow Interior Face.} \\
\begin{align*}
G_{ji^L} \approx \quad &\sum_q w_q (y_1 - y_0) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) \phi^L_{i_0}(-1) \phi^L_{j_1}(x_q) u^L_i\end{align*}

\textbf{Case 2B: Outflow Interior.} \\
\begin{align*}
G_{ji^R} \approx \quad &\sum_q w_q (y_1 - y_0) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) \phi^R_{i_0}(-1) \phi^R_{j_1}(x_q) u^R_i\end{align*}

Now that we set up the semi-discretization in space, we can now focus on time integration. We can evolve \eqref{eq:ode} in time by appling a Runge--Kutta method. In our experiments, we set $N_x = 10, N_y = 10$ and $p=3$. We choose Gauss-Legendre quadrature for Lagrangian elements
\begin{align*}
\phi_i(x) = \prod_{j \neq i} \frac{x - x_j}{x_i - x_j}
\end{align*}
and set
\begin{align*}
&u(x_0, x_1, t) = sin(2 \Pi (x_0 - t)) sin(2 \Pi (x_1 - t))
\end{align*}
with the corresponding Dirichlet condition to define the linear advection system for $0 \leq t \leq T = 1$. Figure \ref{fig:exact} shows different snapshots of the perfect solution. It is evident that the solution at final time and initial time is the same.

\begin{figure}[h!]
    \centering
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.00.png}
        \caption{$t = 0.00$}
    \end{subfigure}
    \hfill
        \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.25.png}
        \caption{$t = 0.25$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.50.png}
        \caption{$t = 0.50$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.75.png}
        \caption{$t = 0.75$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-1.00.png}
        \caption{$t = 1.00$}
    \end{subfigure}

    \caption{Snapshots of the exact solution at selected times.}
    \label{fig:exact}    
\end{figure}

Figure \ref{fig:explicitresults} shows the convergence behaviour for the different explicit integrators of order 1 to 4 from figure \ref{fig:butcherExplicit}. As expected, the higher the order of the method, the bigger is the stable step size for our problem.

\begin{figure}[h!]
    \centering
    % ------------------ Euler ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|c}
        0 & 0 \\ \hline
          & 1
        \end{array}
        \]
        \caption{Explicit Euler (order 1)}
    \end{subfigure}
    \hfill
    % ------------------ RK2 ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|cc}
        0 & 0 & 0 \\
        1 & 1 & 0 \\ \hline
          & \tfrac12 & \tfrac12
        \end{array}
        \]
        \caption{Heun / Explicit RK2 (order 2)}
    \end{subfigure}

    \vspace{1em}

    % ------------------ RK3 ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|ccc}
        0   & 0   & 0 & 0 \\
        \tfrac12 & \tfrac12 & 0 & 0 \\
        1   & -1  & 2 & 0 \\ \hline
            & \tfrac16 & \tfrac23 & \tfrac16
        \end{array}
        \]
        \caption{Classical explicit RK3 (order 3)}
    \end{subfigure}
    \hfill
    % ------------------ RK4 ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|cccc}
        0   & 0    & 0    & 0    & 0 \\
        \tfrac12 & \tfrac12 & 0    & 0    & 0 \\
        \tfrac12 & 0    & \tfrac12 & 0    & 0 \\
        1   & 0    & 0    & 1    & 0 \\ \hline
            & \tfrac16 & \tfrac13 & \tfrac13 & \tfrac16
        \end{array}
        \]
        \caption{Classical explicit RK4 (order 4)}
    \end{subfigure}

    \caption{Butcher tableaus of explicit Runge--Kutta schemes of order 1--4.}
    	\label{fig:butcherExplicit}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{explicit_rk_convergence.png}
    \caption{Convergence of explicit Runge--Kutta methods with 1 to 4 stages.}
    \label{fig:explicitresults}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{rk_explicit_runtime.png}
    \caption{Runtime of explicit Runge--Kutta methods with 1 to 4 stages.}
    \label{fig:explicitruntime}
\end{figure}

A DIRK scheme is an implicit Runge--Kutta method with $A_{ij} = 0$ for all $j>i$. SDIRK methods further fulfill $A_{ii} = A_{jj}$ for all $i,j \leq s$, where $s$ denotes the number of stages. For implicit SDIRK schemes~\cite{kennedy2016diagonally} of order 1 to 4 from figure \ref{fig:butcherImplicit}, we get the result illustrated in figure \ref{fig:implicitresults}.

\begin{figure}[h!]
    \centering

    % ================= Implicit Euler =================
    \begin{subfigure}{\textwidth}
        \centering
        \[
        \begin{array}{c|c}
        1 & 1 \\ \hline
          & 1
        \end{array}
        \]
        \caption{Implicit Euler (order 1)}
    \end{subfigure}

    % ================= SDIRK2 (NCS23) =================
    \begin{subfigure}{\textwidth}
        \centering
        % gamma = (3 + sqrt(3)) / 6
        \[
        \begin{array}{c|cc}
        \gamma     & \gamma & 0 \\
        1-\!2\gamma & 1-\!2\gamma & \gamma \\ \hline
                    & \tfrac12 & \tfrac12
        \end{array}
        \]
        \caption{SDIRK2--NCS23 (order 2)}
    \end{subfigure}

    \vspace{1em}

    % ================= SDIRK3 (NC34) =================
    \begin{subfigure}{\textwidth}
        \centering
        % gamma = (3 + 2 sqrt(3) cos(pi/18)) / 6
        \[
        \begin{array}{c|ccc}
        \gamma        & \gamma & 0 & 0 \\
        \tfrac12-\gamma & \tfrac12-\gamma & \gamma & 0 \\
        2\gamma       & 2\gamma & 1-4\gamma & \gamma \\ \hline
                       & 
            \tfrac{1}{6(1-2\gamma)^2} \quad
            \tfrac{2(1-6\gamma+6\gamma^2)}{3(2\gamma-1)^2} \quad
            \tfrac{1}{6(1-2\gamma)^2}
        \end{array}
        \]
        \caption{SDIRK3--NC34 (order 3)}
    \end{subfigure}
    \hfill

    % ================= SDIRK4 (your gamma) =================
    \begin{subfigure}{\textwidth}
        \centering
        % gamma = 0.435866521508
        \[
        \begin{array}{c|cccc}
        0 & 0 & 0 & 0 & 0 \\[0.2em]
        2\gamma & \gamma & \gamma & 0 & 0 \\[0.2em]
        1 &
            \frac{-4\gamma^2+6\gamma-1}{4\gamma} &
            \frac{-2\gamma+1}{4\gamma} &
            \gamma & 0 \\[0.2em]
        1 &
            \frac{6\gamma-1}{12\gamma} &
            -\frac{1}{(24\gamma-12)\gamma} &
            \frac{-6\gamma^2+6\gamma-1}{6\gamma-3} &
            \gamma \\ \hline
          &
            \frac{6\gamma-1}{12\gamma} \quad
            -\frac{1}{(24\gamma-12)\gamma} \quad
            \frac{-6\gamma^2+6\gamma-1}{6\gamma-3} \quad
            \gamma
        \end{array}
        \]
        \caption{4-stage SDIRK (order 4), $\gamma = 0.435866521508$}
    \end{subfigure}

    \caption{Butcher tableaux for implicit Euler and SDIRK methods of order 2--4.}
    \label{fig:butcherImplicit}
\end{figure}
    
\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{rk_implicit_convergence.png}
    \caption{Convergence of implicit Runge--Kutta methods with 1 to 4 stages.}
    \label{fig:implicitresults}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{rk_implicit_runtime.png}
    \caption{Runtime of implicit Runge--Kutta methods with 1 to 4 stages.}
    \label{fig:implicitruntime}
\end{figure}

In order to speed up the implicit integrators, we would like to use the matrix-free tensor-product preconditioners of Pazner~\cite{pazner2018approximate} and Diosady~\cite{diosady2017tensor}. We begin with implementing the former for matrix-based code. Notice that we do not exploit the efficiency advantages of a matrix-free tensor-product implementation yet, but only focus on the accuracy to discuss their potential.

\subsection*{A Kronecker-SVD Based Preconditioner}
When solving \eqref{eq:ode} with a DIRK integrator, we get an implicit system of the form 
\begin{align*}
(M - h a_{j,j} (G - B)) U_j = M U_0 + h a_{j,j} G_{\textnormal{bound}}(t_j) + h \sum^{j-1}_{l=1} a_{j,l} [(G-B) U + G_{\textnormal{bound}}(t_l)]
\end{align*}
at each stage $j$. Notice that the right-hand side can be explicitly computed, such that we arrive at a linear system. Notice that for a SDIRK method, it holds that $a_{j,j} = a_{l,l}$ for all $j,l$ such that we always have the same operator on the left, thus can always use the same preconditioner and precompute it only once.

Pazner~\cite{pazner2018approximate} proposes to use an element-wise rank-2 Kronecker SVD to do so, i.e. for $A_e = M_e - h a_{j,j} (G_e - B_e)$ we estimate
\begin{align*}
A_e \approx \sum^r_{j=1} A_j \otimes B_j,
\end{align*}
which can be computed via an SVD of a shuffled version $\tilde{A}_e$ of $A_e$. The full SVD's cost scales cubicly in the size of the matrix to be estimated. Luckily, there are ways to avoid this, namely the Lanczos SVD and the Randomized SVD. Both reduce the runtime to near squared complexity and deliver near-optimal accuracy with high probability. In his paper, Pazner decides to use the Lanczos algorithm.

In our experiments in the same setting as earlier, this yields the following convergence behaviour. Accuracy-wise, we expectedly get the same results as we only changed the preconditioner, see figure \ref{fig:paznerresults}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{average_iterations_implicit.png}
    \caption{Convergence of implicit Runge--Kutta methods with 1 to 4 stages combined with Pazner's preconditioner.}
    \label{fig:paznerresults}
\end{figure}

In terms of runtime, we cannot recognise significant speed-up as demonstrated in figure \ref{fig:paznertime}.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{rk_implicit_runtime_precon.png}
    \caption{Runtime of implicit Runge--Kutta methods with 1 to 4 stages combined with Pazner's preconditioner.}
    \label{fig:paznertime}
\end{figure}

Even though we could show Pazner's preconditioner works in terms of accuracy, the results could not convince with respect to runtime. However, this is probably because a final time of 0.1 and a number of elements as small as 100 do not rectify the extra expense of assembling the preconditioner because figure \ref{fig:implicititer} and \ref{fig:pazneriter} show that the preconditioner is able to reduce the number of iterations significantly. This is definitely a better indicator as the implementations provided are not optimized, but rather educational.

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{average_iterations_implicit.png}
    \caption{Average number of iterations for implicit Runge--Kutta methods with 1 to 4 stages without preconditioning.}
    \label{fig:implicititer}
\end{figure}

\begin{figure}[h!]
    \centering
    \includegraphics[width=0.85\textwidth]{average_iterations_precon.png}
    \caption{Average number of iterations for implicit Runge--Kutta methods with 1 to 4 stages with Pazner's preconditioner.}
    \label{fig:pazneriter}
\end{figure}

Notice that we display the average number of iterations per stage. For the 4-stage ESDIRK method with an explicit first stage, we thus divide by 4 even though we only have to solve an implicit system for 3 stages. This is why GMRES appears to perform much better here.

\subsection*{Outlook}
This is a work in progress. Next, we plan on implementing the FDM preconditioner for the matrix case in order to conduct a thorough comparison of tensor-product preconditioners. We will then try out a randomized preconditioner based on the RSVD. Depending on the results, an implementation in C++ deal.II will follow as well as an extension to three-dimensional problems.

%%% Bibliography
\bibliographystyle{alpha}
\bibliography{references}
%%%

\end{document}
