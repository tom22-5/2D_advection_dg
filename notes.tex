\documentclass[12pt]{report}
%%% Packages
\usepackage[a4paper, margin=1in]{geometry}
\usepackage{amsmath, amssymb}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage[numbers]{natbib}
\usepackage{titlesec}
\usepackage{tocloft}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{multirow}
\usepackage{array}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{booktabs}
\usepackage{color}
\usepackage{graphicx}
\usepackage{subcaption}
%%%

\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}
\newtheorem{lemma}{Lemma}

\theoremstyle{remark}
\newtheorem{remark}{Remark}

%%%
\setlength{\parindent}{0pt}


\begin{document}

\title{Tensor-Product Preconditioner For Very High Order Discontinuous Galerkin Discretizations}
\author{Tom Feldhausen}
\date{\today}

\maketitle

\begin{abstract}
Solving a multi-dimensional PDE by means of a Discontinuous Galerkin method in a higher order solution space requires the solution of large linear systems during time integration. When storing the semi-discrete system, communication dominates the computational costs such that matrix-free implementations turn out to be more efficient in practice. In order to efficiently integrate implicit time integration into this framework, we need matrix-free preconditioners. This project aims to compare the tensor-product preconditioners of Diosady~\cite{diosady2017tensor} and Pazner~\cite{pazner2018approximate} for the example of linear advection and provide an optimized C++ deal.II~\cite{arndt2024deal} implemenation. As a first step, we provide a basic matrix-based DG implementation of two-dimensional linear advection with upwind flux.
\end{abstract}

\section*{A Matrix-Based Discontinuous Galerkin Implementation For Two-Dimensional Linear Advection In Python}
\textit{The code for all experiments is provided in \cite{2D_advection_dg}.} \\
As a first step, we provide a matrix-based DG discretization for the two-dimensional linear advection system 
\begin{align*}
&\partial_t u(x, t) + a(x,t) \cdot \nabla u(x,t) = 0 \quad \forall x \in [0,1]^2, \quad 0 \leq t \leq T
\\
&u(x, 0) = u_0(x) \quad \forall x \in [0,1]^2
\\
&u(x, t) = u_{\textnormal{in}}(x) \quad \forall x \in \partial [0, 1]^2
\end{align*}
for a solution function $u$. Following Kronbichler and Persson~\cite{kronbichler2021efficient}, we arrive at the weak formulation that for all $v \in H^1(\Omega)$ it needs to hold that
\begin{align}
\label{eq:weakform}
\int_\Omega \partial_t u v - \int_\Omega u a \cdot \nabla v + \int_{\partial \Omega} (a \cdot n) u v = 0,
\end{align}
after integrating in space, multiplying by a test function $v$ and integrating by parts.

Now instert basis functions of order $p$, i.e.
\begin{align*}
u = \sum^p_{i=0} u_i \Phi_i, v = \sum^p_{j=0} v_j \Phi_j.
\end{align*}
Then condition \eqref{eq:weakform} is equivalent to the element-wise formulation for each basis function, i.e.
\begin{align}
\int_{\Omega_e} \partial_t u_i(t) \Phi_i \Phi_j - \int_{\Omega_e} u_i \Phi_i a \cdot \nabla \Phi_j + \int_{\partial \Omega_e} (a \cdot n) u_i \Phi_i \Phi_j = 0
\end{align}
being true for all $0 \leq i,j \leq p$ and $\Omega_e \subset \Omega$.

Analogously we can write this in matrix form, which yields the system
\begin{align*}
\label{eq:ode}
\frac{dU(t)}{dt} = M^{-1} [(B + G) U + G_{\textnormal{bound}}].
\end{align*}
We consider a tensor-product mesh with $N = N_x \times N_y$ rectangular elements. Then $x: \Omega_{\textnormal{ref}} \rightarrow \Omega_e$ maps from the reference element $\Omega_{\textnormal{ref}} = [-1,1]^2$ to the actual element $\Omega_e = [x_0, y_0] \times [x_1, y_1]$ and can be explicitly written as 
\begin{align*}
\mathbf{x}(\boldsymbol{\zeta})
=
A\,\boldsymbol{\zeta} + \mathbf{b},
\qquad
A = \frac{1}{2}
\begin{pmatrix}
x_1 - x_0 & 0 \\
0 & y_1 - y_0
\end{pmatrix},
\qquad
\mathbf{b} = 
\begin{pmatrix}
\frac{x_1 + x_0}{2} \\
\frac{y_1 + y_0}{2}
\end{pmatrix}.
\end{align*}

With this at hand, we can explicitly write out the mass matrix as
\begin{align*}
M_{ji} = \quad &\int_{\Omega_e} \Phi_j(x) \Phi_i(x) dx
\\ 
= \quad &\int^{y_0}_{x_0} \int^{y_1}_{x_1} \Phi_j(x_0, x_1) \Phi_i(x_0, x_1) dx_0 dx_1 
\\
= \quad &\int^1_{-1} \int^1_{-1} \Phi_j(\zeta_1, \zeta_2)  \Phi_i(\zeta_1, \zeta_2) \|\operatorname{det}(J^{-1}_e)\| d\zeta_1 d\zeta_2.
\end{align*}

Using Gauss-Legendre quadrature with one-dimensional weights $w_q$ and points $x_q$, we get the approximation
\begin{align*}
M_{ji} = \quad &\int_{\Omega_e} \Phi_j(x) \Phi_i(x) dx
\\ 
\approx \quad &\sum_{q_0} \sum_{q_1} w_{q_0} w_{q_1} \phi_{i_1}(x_{q_0}) \phi_{i_2}(x_{q_1}) \phi_{j_1}(x_{q_0}) \phi_{j_2}(x_{q_1}) \|(y_0 - x_0) (y_1 - x_1)\|
\end{align*}
when inserting the tensor-product form $\Phi_i = \phi_{i_1} \phi_{i_2}$ and exploiting $\|\operatorname{det}(J^{-1}_e)\| = \|(y_0 - x_0) (y_1 - x_1)\|$.

Similarly, we can derive an expression for the volume matrix and calculate
\begin{align*}
B_{ji} = \quad &\int_{\Omega_e} \Phi_i (a \cdot \nabla \Phi_j)
\\
= \quad &\int^{y_0}_{x_0} \int^{y_1}_{x_1} \Phi_i(z_0, z_1) (a(z_0, z_1, t) \cdot \nabla \Phi_i(z_0, z_1)) dz_0 dz_1 
\\
= \quad &\int^{y_0}_{x_0} \int^{y_1}_{x_1} \Phi_i(z_0, z_1) (a_0(z_0, z_1, t) \partial_{z_0} \Phi_i(z_0, z_1) + a_1(z_0, z_1, t) \partial_{z_1} \Phi_i(z_0, z_1)) dz_0 dz_1 
\\
= \quad &\int^{-1}_{1} \int^{-1}_{1} \Phi_i(\zeta_0, \zeta_1) (a_0(\zeta_0, \zeta_1, t) \partial_{z_0} \Phi_i(\zeta_0, \zeta_1) + a_1(\zeta_0, \zeta_1, t) \partial_{z_1} \Phi_i(\zeta_0, \zeta_1)) 
\\ 
\quad &\|(y_0 - x_0) (y_1 - x_1)\| d\zeta_0 d\zeta_1
\\
= \quad &\sum_{q_0} \sum_{q_1} w_{q_0} w_{q_1} \phi_{i_1}(x_{q_0}) \phi_{i_2}(x_{q_1}) (a_0(x_{q_0}) \phi'_{j_1}(x_{q_0}) + a_1(x_{q_0}) \phi_{j_1}(x_{q_0}) \phi'_{j_2}(x_{q_1})) \phi_{j_2}(x_{q_1}) 
\\
\quad &\|(y_0 - x_0) (y_1 - x_1)\|.
\end{align*}

The face terms are slightly more difficult to derive as in two dimensions we have four faces that are either interior and face other elements or that are part of the domain boundary. This is why for a specific derivation, we need to choose an explicit flux and face. In this example, we choose an upwind flux for information flow between elements, i.e. 
\begin{align*}
\widehat{au}
=
\begin{cases}
(a \cdot n) u^{-}, & a \cdot n > 0, \\[6pt]
(a \cdot n) u^{+}, & a \leq 0
\end{cases}
\end{align*}
and focus on the left face. As this is one-dimensional, we will only need the $x_1(\zeta)$ part of the transformation, that concerns the $x_1$-axis. The corresponding determinant of the inverse Jacobian is simply $(y_1 - x_1)$ for our tensor-product grid. Generally it holds that 
\begin{align*}
&\int_{\partial \Omega_e} \widehat{au}(\Phi_i^R(z), \Phi_i^L(z)) \Phi_j(z) dz
\\
= \quad &\int^1_{-1} (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(\zeta) \widehat{au}(\Phi^R_i(zeta), \Phi^L_j(\zeta)) d\zeta
\\
\approx \quad &\sum_q w_q (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(x_q) \widehat{au}(u^R_i \phi^R_{j_0}(-1) \phi^R_{j_1}(x_1(x_q)), u^L_i \phi^L_{j_0}(-1) \phi^L_{j_1}(x_1(x_q))).
\end{align*}

The value of $\widehat{au}(u^R_i \phi^R_{j_0}(-1) \phi^R_{j_1}(x_1(x_q)), u^L_i \phi^L_{j_0}(-1) \phi^L_{j_1}(x_1(x_q)))$ depends on the following cases. 

\textbf{Case 1A: Inflow Boundary.} \\
\begin{align*}
{G_{\textnormal{bound}}}_j \approx \quad &\sum_q w_q (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) u(x_0(-1), x_1(x_q), t)
\end{align*}

\textbf{Case 1B: Outflow Boundary.} \\
\begin{align*}
G_{ji^R} \approx \quad &\sum_q w_q (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) \phi^R_{i_0}(-1) \phi^R_{j_1}(x_q) u^R_i
\end{align*}

\textbf{Case 2A: Inflow Interior Face.} \\
\begin{align*}
G_{ji^L} \approx \quad &\sum_q w_q (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) \phi^L_{i_0}(-1) \phi^L_{j_1}(x_q) u^L_i\end{align*}

\textbf{Case 2B: Outflow Interior.} \\
\begin{align*}
G_{ji^R} \approx \quad &\sum_q w_q (y_1 - x_1) \phi_{j_0}(-1) \phi_{j_1}(x_q) a(x_0(-1), x_1(x_q), t) \phi^R_{i_0}(-1) \phi^R_{j_1}(x_q) u^R_i\end{align*}

Now that we set up the semi-discretization in space, we can now focus on time integration. We can evolve \eqref{eq:ode} in time by appling a Runge--Kutta method. In our experiments, we set $N_x = 20, N_y = 20$ and $p=2$. We choose Lagrangian elements
\begin{align*}
\phi_i(x) = \prod_{j \neq i} \frac{x - x_j}{x_i - x_j}
\end{align*}
and set
\begin{align*}
&u(x_0, x_1, t) = sin(2 \Pi (x_0 - t)) sin(2 \Pi (x_1 - t))
\end{align*}
with the corresponding Dirichlet condition to define the linear advection system for $0 \leq t \leq T = 1$. Figure \ref{fig:exact} shows different snapshots of the perfect solution. It is evident that the solution at final time and initial time is the same.

\begin{figure}[h!]
    \centering
    \label{fig:exact}

    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.00.png}
        \caption{$t = 0.00$}
    \end{subfigure}
    \hfill
        \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.25.png}
        \caption{$t = 0.25$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.50.png}
        \caption{$t = 0.50$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-0.75.png}
        \caption{$t = 0.75$}
    \end{subfigure}
    \hfill
    \begin{subfigure}{0.32\textwidth}
        \centering
        \includegraphics[width=\linewidth]{snapshot-1.00.png}
        \caption{$t = 1.00$}
    \end{subfigure}

    \caption{Snapshots of the exact solution at selected times.}
\end{figure}

Figure \ref{fig:explicitresults} shows the convergence behaviour for the different explicit integrators of order 1 to 4 from figure \ref{fig:butcherExplicit}. As expected, the higher the order of the method, the bigger is the stable step size for our problem.

\begin{figure}[h!]
    \centering
	\label{fig:butcherExplicit}
    % ------------------ Euler ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|c}
        0 & 0 \\ \hline
          & 1
        \end{array}
        \]
        \caption{Explicit Euler (order 1)}
    \end{subfigure}
    \hfill
    % ------------------ RK2 ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|cc}
        0 & 0 & 0 \\
        1 & 1 & 0 \\ \hline
          & \tfrac12 & \tfrac12
        \end{array}
        \]
        \caption{Heun / Explicit RK2 (order 2)}
    \end{subfigure}

    \vspace{1em}

    % ------------------ RK3 ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|ccc}
        0   & 0   & 0 & 0 \\
        \tfrac12 & \tfrac12 & 0 & 0 \\
        1   & -1  & 2 & 0 \\ \hline
            & \tfrac16 & \tfrac23 & \tfrac16
        \end{array}
        \]
        \caption{Classical explicit RK3 (order 3)}
    \end{subfigure}
    \hfill
    % ------------------ RK4 ------------------
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|cccc}
        0   & 0    & 0    & 0    & 0 \\
        \tfrac12 & \tfrac12 & 0    & 0    & 0 \\
        \tfrac12 & 0    & \tfrac12 & 0    & 0 \\
        1   & 0    & 0    & 1    & 0 \\ \hline
            & \tfrac16 & \tfrac13 & \tfrac13 & \tfrac16
        \end{array}
        \]
        \caption{Classical explicit RK4 (order 4)}
    \end{subfigure}

    \caption{Butcher tableaus of explicit Runge--Kutta schemes of order 1--4.}
\end{figure}

\begin{figure}[h!]
    \centering
    \label{fig:explicitresults}
    \includegraphics[width=0.85\textwidth]{explicit_rk_convergence.png}
    \caption{Convergence of explicit Runge--Kutta methods with 1 to 4 stages.}
    \label{fig:explicit-rk-convergence}
\end{figure}

For implicit DIRK schemes~\cite{kennedy2016diagonally} of order 1 to 4 from figure \ref{fig:butcherImplicit}, we get the result illustrated in figure \ref{fig:implicitresults}.

\begin{figure}[h!]
    \centering
    \label{fig:butcherImplicit}

    % ================= Implicit Euler =================
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|c}
        1 & 1 \\ \hline
          & 1
        \end{array}
        \]
        \caption{Implicit Euler (order 1)}
    \end{subfigure}
    \hfill
    % ================= Trapezoidal Rule =================
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|cc}
        0 & 0   & 0 \\
        1 & \tfrac12 & \tfrac12 \\ \hline
          & 0 & \tfrac12
        \end{array}
        \]
        \caption{Trapezoidal Rule (order 2)}
    \end{subfigure}

    \vspace{1em}

    % ================= DIRK3 =================
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|ccc}
        0     & 0           & 0            & 0 \\
        \tfrac32 & \tfrac34  & \tfrac34     & 0 \\
        1     & \tfrac{7}{18} & -\tfrac{4}{18} & \tfrac{15}{18} \\ \hline
              & \tfrac{7}{18} & -\tfrac{4}{18} & \tfrac{15}{18}
        \end{array}
        \]
        \caption{DIRK3 (order 3)}
    \end{subfigure}
    \hfill
    % ================= DIRK4 =================
    \begin{subfigure}{0.45\textwidth}
        \centering
        \[
        \begin{array}{c|ccccc}
        0 & 0 & 0 & 0 & 0 & 0 \\
        \tfrac32 & \tfrac34 & \tfrac34 & 0 & 0 & 0 \\
        \tfrac75 & \tfrac{447}{675} & -\tfrac{357}{675} & \tfrac{855}{675} & 0 & 0 \\
        1 & \tfrac{13}{42} & \tfrac{84}{42} & -\tfrac{125}{42} & \tfrac{70}{42} & 0 \\ \hline
          & \tfrac{13}{42} & \tfrac{84}{42} & -\tfrac{125}{42} & \tfrac{70}{42} & 0
        \end{array}
        \]
        \caption{DIRK4 (order 4, 5-stage)}
    \end{subfigure}

    \caption{Butcher tableaus for the implicit Euler, trapezoidal rule, DIRK3, and DIRK4 time integration methods.}
\end{figure}

\begin{figure}[h!]
    \centering
    \label{fig:implicitresults}
    \includegraphics[width=0.85\textwidth]{rk_implicit_convergence.png}
    \caption{Convergence of implicit Runge--Kutta methods with 1 to 4 stages.}
    \label{fig:implicit-rk-convergence}
\end{figure}

In order to speed up the implicit integrators, we would like to use the matrix-free tensor-product preconditioners of Pazner~\cite{pazner2018approximate} and Diosady~\cite{diosady2017tensor}. We begin with implementing the former for matrix-based code. Notice that we do not exploit the efficiency advantages of a matrix-free tensor-product implementation yet, but only focus on the accuracy to discuss their potential.

\subsection*{A Kronecker-SVD Based Preconditioner}
When solving \eqref{eq:ode} with a DIRK integrator, we get an implicit system of the form 
\begin{align*}
(M - h a_{j,j} (G - B)) U_j = M U_0 + h a_{j,j} G_{\textnormal{bound}}(t_j) + h \sum^{j-1}_{l=1} a_{j,l} [(G-B) U + G_{\textnormal{bound}}(t_l)]
\end{align*}
at each stage $j$. Notice that the right-hand side can be explicitly computed, such that we arrive at a linear system. Notice that for a SIRK method, it holds that $a_{j,j} = a_{l,l}$ for all $j,l$ such that we always have the same operator on the left, thus can always use the same preconditioner and precompute it only once.

Pazner~\cite{pazner2018approximate} proposes to use a rank-2 Kronecker SVD to do so, i.e. for $A = M - h a_{j,j} (G - B)$ we estimate
\begin{align*}
A \approx \sum^r_{j=1} A_j \otimes B_j,
\end{align*}
which can be computed via an SVD of a shuffled version $\tilde{A}$ of $A$. The full SVD's cost scales cubicly in the size of the matrix to be estimated. Luckily, there are ways to avoid this, namely the Lanczos SVD and the Randomized SVD. Both reduce the runtime to near squared complexity and deliver near-optimal accuracy with high probability. In his paper, Pazner decides to use the Lanczos algorithm.

\subsection*{Outlook}
This is a work in progress. Next, we plan on implementing the FDM preconditioner for the matrix case in order to conduct a thorough comparison of all methods discussed. We will then try out a randomized preconditioner based on the RSVD. Depending on the results, an implementation in C++ deal.II will follow as well as an extension to three-dimensional problems.

%%% Bibliography
\bibliographystyle{alpha}
\bibliography{references}
%%%

\end{document}
